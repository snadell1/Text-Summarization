The authors extend earlier work (Wiering and van Hasselt 2008) on ensemble methods for reinforcement learning and offer both qualitative and empirical support for the effectiveness of their approach. The paper begins by highlighting the drawbacks of using a single RL algorithm chosen for and/or tuned to a particular case - specifically, that it defeats some of the purpose of autonomous learning and can't be done in the real world as it requires multiple iterations to find a good algorithm for a domain. They propose an ensemble approach, inspired by the success of such approaches in other areas of AI, in which a meta-learner learns from the Q-values of base level RL agents and minimizes the Bellman error. They point out the novelty of this over Wiering and van Hasselt 2008, which is that it combines results of base level agents based on their actual performance as opposed to a fixed rule. 

Later portions of the paper show results of this approach in the mountain-car environment; the methodology and technical rigor seem sound, if somewhat preliminary (only one environment) and limited in detail (ex. as to the details of how base level agents were tweaked and how this may have affected the results). Additionally, the authors suggest several avenues for further research on ensemble RL. Overall, the paper is very clear, makes a novel contribution, and could support a full length presentation, though the reviewer has relatively low familiarity with RL literature and ensemble approaches in other areas, so I can only recommend accepting with medium confidence and cannot nominate it for best paper.