In this paper Rawlik et al. formalize the duality between Stochastic Optimal Control and Probabilistic inference so that efficient methods from the probablistic inference domain can be transferred to solve finite and infinite horizon SOC problems. Based on this dual reformulation of SOC problems they introduce two iterative methods for solving SOC: Xi-Iterations and Posterior Policy Iterations(PPI).

It's shown in the paper that the dual problem is finding the policy which matches the controlled process in SOC domain with the posterior process in probabilistic inference domain and this policy will be the optimal policy. Thus optimal policy is the one that minimizes the KL divergence between controlled policy and posterior policy. The new iterative methods introduced have global convergence. For Xi-iterations, the convergence rate increases with eta and it is experimentally shown that it converges in fewer iterations than Q-learning algorithm. In PPI, the objective being minimized for obtaining optimal policy is risk and thus we obtain near optimal risk neutral policies for SOC problems. It's experimentally shown that reducing value of eta leads to policies obtained by approximate inference.