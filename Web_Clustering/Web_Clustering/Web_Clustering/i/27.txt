In this paper the authors address the problem of transfer learning in sentiment classification. The authors analyze sentiment across four domains, and show that a model trained on one domain is not suitable for sentiment classification on another domain. The authors then propose an active-learning based approach to update a model to get better performance in the new domain. Their solution is a Query by Committee (QBC) approach. The approach uses a committee of two classifiers, one trained on the source domain data and one trained on the target domain data, to label the samples. It then selects the samples which the committee has the most disagreement for human annotation.

The authors address an important question in sentiment classification. Often, when sentiment is calculated in a new domain it is a daunting task to create a new sentiment lexicon appropriate for this domain. Creating this lexicon can be a time-consuming task and may require the time of subject matter experts. Alleviating as much of this process as possible by classifying instances the committee agrees upon makes sense. 

The first experiment they run in the paper is comparing the efficacy of their label propagation algorithm with other state-of-the-art domain adaptation algorithms. They find that they beat the state of the art in 7/12 experiments with their source classifier and 2/4 in their target classifier. This is only OK, however this is not the bread and butter of their approach. Their next experiment combines all parts into the full algorithm including active learning.

The more interesting part of this paper is section 5.3 where the authors test their QBC approach with some classifiers. It is interesting to see the proposed classifier performs better in cross-domain situations where the two domains are unrelated (for example, QBC is the clear winner in “Kitchen -> DVD”, however it more or less ties in “Electronic -> DVD”).  I would like to see more discussion of why it underperformed in some domain transfer tasks and overperformed in others. What characteristics of the data may have led to this result?

In the experiment section the authors use unigrams and bigrams as features, and only use a maximum entropy model for classification. It would have been nice to see a comparison of different classifiers. It might be interesting to try different n-grams, and see which value of n gives the best results.

In Tables 2, 3 it is unclear what you are using for the baseline. More explanation is needed. One baseline to consider is simply training on only labeled data from the target domain. 

Overall, I think their methodology is communicated clearly.  Their results are underwhelming overall, however it seems to be a marginal contribution over the state-of-the-art approaches in active learning. Their use of figures along with pseudocode helps make clear what they are doing. Their problem, while important, is not novel while their approach is. Because of this I think this paper is suitable as a poster paper.