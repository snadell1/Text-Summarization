In this paper the authors propose a solution to solving large-scale sparse MDPs using reinforcement learning. By providing a smoothing estimate for unseen state/action pairs and a novel exploration heuristic, the authors propose a new algorithm to solve the MDP. Overall, this paper’s highly theoretical findings are very interesting for the AI community, but more importantly for someone taking CSE 571. This paper’s focus on the construction of the reinforcement learning task along with their motivation of their exploration function really paint the picture for how reinforcement learning is done in the real world.

Because this is reinforcement learning, they do not know the transition probabilities and have to learn them. One of the main contributions of this paper is solving sparse MDPs, those where many transition probabilities are 0 (P(s’| s, a) = 0). They provide a bound for the probability of unseen transitions based on the Good-Turing estimate, a common smoothing technique. They incorporate this into the already-proposed “UpperP” algorithm to estimate Q- and V- values. One thing that I would like to see in this work is a comparison of other smoothing techniques (perhaps Kernel Density Estimation, or even Laplace smoothing) and see how these fare in an experiment. 

Another important contribution to this paper is the exploration function for their algorithm. The heuristic is based on “occupancy measure”: the probability that the optimal policy will cause the agent to visit that state. The exploration function then chooses the action that maximizes the probability of visiting that state. I think from a philosophical perspective this makes sense. This does not do exploration for the sake of exploration, but rather for the sake of finding the optimal policy.

Using these constructs the authors construct the DDV algorithm. This algorithm works by looping until the policy converges. At each step it re-estimates the bounds on the Q- and V-values, chooses and simulates an action. The authors also prove that this algorithm can converge in polynomial time.

The authors back up their theoretical findings with some experiments. They test their algorithm with a toy MDP that they construct. They show that their Good-Turing estimates for the Q-values cause the algorithm to converge faster. Finally, they show that their algorithm works well on their invasive species problem.

My strongest criticism of this paper is that they really skip over some of the more important steps in building their MDP. While I have an understanding of their general approach, it would be nice to have more detail towards how they actually apply it to their problem. However, we have to remember that this is probably due to the page limit imposed by the conference.

Overall, this is a strong theoretical paper that really sheds light on how reinforcement learning is carried out in the real world. Because of this, I think it is a great candidate for publication in our mock conference.