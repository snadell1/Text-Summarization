In reinforcement learning, the fundamental objective is to maintain a proper balance between exploration and exploitation. It will be very challenging when the agent can only partially observe the states of its environment. This paper focuses on reinforcement learning in pomdps. An online nested expectation maximization algorithm that evaluates the policy only in the current learning episode by memorizing only the sufficient statistics is proposed. The paper claims that this algorithm has reduced its time complexity from O(n*n) to O(n) and space complexity from O(n) to O(1) compared to the batch-mode algorithm. The optimal policy for pomdp can be obtained by solving the MDP in its belief states. In case of reinforcement learning, the policy has to be learned while experiencing the pomdp. The policy in reinforcement learning can be learned in two ways: model-based reinforcement learning and model-free reinforcement learning. Since the underlying model is pomdp, learning a model can help to address other issues as well apart from just learning the policy. But learning the model can be very time consuming. Other approaches like model-free learning doesn't require the underlying model to be learned but focuses on finding the optimal policy which can be more time efficient, but less versatile. If the policy is of primary importance, then learning the model is required. But there are some approximation algorithms which can enhance the time efficiency by reducing the model's versatility.

Finite state controllers (FSC) provide a simple, convenient way of representing policies for partially observable Markov decision processes. Regionalized policy representation (RPR) is a collection of policies that treat belief state as a latent random variable that is integrated to find the policy. RPR is a general form of FSC, where each internal memory unit is associated with a distribution of actions, instead of a single action. The optimal policy of a pomdp can be represented by a RPR as it is an FSC in special case which is known to approximate the optimal policy of the pomdp. Nested expectation-maximization algorithm can be used to learn the optimal RPR. Earlier approaches using this algorithm assume batch-mode learning that updates the policy based on the entire set of experiences collected so far. In the algorithm proposed, policy is updated each time based on a partial policy evaluation. So, each episode's contribution to the sufficient statistics is computed only when the episode is new, instead of computing repeatedly as each episode comes in. This makes the expectation step computationally more efficient. Also, as each episode is used only when it is new, there is no need to remember all the older ones but just the current latest episode, leading to a higher memory efficiency.

The algorithm essentially performs an on-policy learning where the agent learns the RPR while also using it to choose the actions in collecting experiences. They use the dual-policy approach to maintain balance between exploration and exploitation using both RPR policy and random policy that is similar to the epsilon-greedy policy. Initially consider a random policy and recompute the rewards in the outer E-loop and keep updating the RPR till the inner-loop EM converges. Now compute the new policy using exploration and exploitation approach used in this paper. Adjust learning rate and keep iterating for n loops. This gives the optimal policy in the form of RPR.

The results have shown that online algorithm proposed requires very less space and time complexities compared to the batch approach. The CPU time compared to batch approach is much smaller. Rewards acheived by the online algorithm are much higher than the batch algorithm with a constraint on the number of episodes. Which means, within a short time, online algorithm is able to reach the optimal performance given by the batch algorithm. Also, the online EM algorithm doesn't get stuck at the local optima as it is a stochastic algorithm that is capable of jumping out of local optima to obtain better solutions. There is a trade-of between the number of episodes and the policy quality. For few benchmark problems considered in the paper for lesser number of episodes, batch algorithm is better than the online algorithm but online algorithm makes the RPR scale to larger problems. These benchmark problems are the standard pomdp parameter files that have appeared in the previous literature. Also, the online algorithm is compared with other previous approaches on reinforcement learning in pomdps like U-Tree, iPOMDP, MDP-EM and found that online algorithm is much faster to achieve higher rewards and also requires less decision states compared to the other approaches. This paper also provided a strong correlation between their hypothesis and the results obtained as shown in the experimental section. The paper is clearly well-written but there are few wrong references (figures/algorithms) and grammatical errors. Overall, it is an interesting approach and it would have been better to see how the performance of the algorithm is varying with varying number of states > 40.