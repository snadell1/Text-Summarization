This paper presents a novel approach to min-max search in stochastic two-player zero-sum games using Monte Carlo simulation to apply a sparse-sampling technique, indicating a consistent increase in game-performance over classic *-Minimax approaches and comparable performance to popular, modern approaches such as Monte Carlo Tree Search (MCTS). The approach is applied in densely stochastic game domains where chance nodes with high stochasticity present exponentially increasing branching factor in the search space, presenting the need for an efficient, reasonably accurate evaluation function with low search depth. As the paper indicates, literature in the application of the Monte Carlo approach is scarce, presenting an opportunity for evaluation of the technique. Specifically, the paper presents a Monte-Carlo based sampling technique for the outcomes of chance nodes. The paper discusses the general classes of algorithms applied in zero-sum game domains, indicating how the *-Minimax algorithm is modified to improve its performance. Finally, the proposed Monte Carlo *-Minimax Search (MCMS) algorithm is evaluated on four example cases of adversarial zero-sum games: Pig, EinStein Wurfelt Nicht! (EWN), Can’t Stop, and Ra. Results indicated that the proposed algorithm performs well in general, with the exception of EWN, in which case the derived evaluation function was poor in quality and impacted performance. In terms of playing strength, MCMS performs competently relative to MCTS and significantly better than its classic counterpart.

 	Several qualities in the strength of this work make it a strong and thorough defense for the claim that MCMS is an effective playing algorithm in densely stochastic domains. The paper clearly addresses the issues presented by this domain for search algorithms that rely on reasonable branch factoring with respect to increasing horizon on nodes. The success of the sparse sampling approach in MDPs is used as a basis for application in game search, and the reasoning used (that the size of the constant c of successor states is independent from the number of states) effectively justifies the use of this method for sampling on chance nodes. Furthermore, the paper states that only a small value for the constant c is necessary to obtain reasonable performance, and this claim is reflected well in the results of the experiments. The game domains used in the experimentation form a reasonable set of example cases of densely-stochastic games, and their relative popularity, as the paper explains, qualifies them as target domains for evaluation of MCMS. The algorithms are compared against the recent MCTS approach, which has received some acclaim for its performance in the field and serves as a suitable standard of comparison. Finally, the paper is thorough in its coverage of the general format of game tree search algorithms, as well as the specific weaknesses of classic approaches (intractability of look-aheads on chance node outcomes) that provide a concrete need for the proposed sampling methods. The approach’s novelty is clearly established, background evidence is adequately provided, and the technical content of the writing is concisely presented and well supported by the evidence given.

	Having established this, there are a few aspects of the approach and descriptions given which require further explanation to improve the clarity of the work. For instance, there is a claim that the enhancement over search tree widening done in DPW improves the performance of MCTS in densely stochastic domains (at the end of Section 2.2: Monte Carlo Tree Search) that requires a reference from literature to support it. It is not intuitively obvious how the double progressive widening method allows MCTS to perform more effectively under dense stochasticity, and an elaboration on this point or a reference to previous work would help support this point. Furthermore, the decisions made on the values of the parameters used during experimentation under Section 4.2: Experimental Setup would benefit from brief explanation to provide clarity on the experimental method. For instance, the indication that “low-overhead static move orderings are used to enumerate actions” (p5) is a specific restriction on the application space that may influence the generalizability of the results generated. In addition, the paper specifies exact, chosen values for several parameters of the experiment, including a time limit on the search of 200 ms, and a constant for the total number of runs on each algorithm (50), without providing supporting evidence justifying the chosen parameters. How were the selected values for these parameters derived, and how might the results have been affected by changing these values? 

        The data indicates significantly poorer performance on EWN relative to the other game domains; the paper claims that this is due to a poor quality in the evaluation function in EWN, but this explanation is insufficient to justify the deviation in performance in this case, as it implies that there may be an unknown factor related to the structure of a game domain that interferes with the algorithms’ performance. Further investigation into the potential factors leading to the poorer performance in EWN is potentially worthy of investigation, as it would bolster the reliability of the proposed method, and may be a suitable target for future work. Finally, it is important to note that, while the set of example games used for evaluation is adequate in size and span, further evaluation of the algorithm’s flexibility in various cases is required before the generalized claim that “MCMS is well suited to densely stochastic games” (p6) can be completely validated. It may benefit the work to clarify that further testing on MCMS is necessary to support its application in practice. Overall, the paper presents a very exciting premise for the future of search algorithms in the stochastic gaming field, and is more than worthy of recognition as a powerful contribution to this area.