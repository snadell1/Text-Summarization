The authors of the paper, The Dynamics of Reinforcement Social Learning in Cooperative Multiagent System, talk about variety of coordination problems involved in multiagent systems (MASs) in which the agents share the same goals and reward functions. The authors claim that most of the multiagent reinforcement algorithms which rely on Q-Learning majorly, try to find an optimal coordination policy through repeated interactions with the same agents. In practical scenarios this seldom applies in MASs and it is highly unlikely to have the same agents to collaborate with each other all the time.   This methodology of learning the policy by interacting with different agents is known as Social Learning.

Though there have been many algorithms proposed to address this and none of them have come close to solve a fully stochastic social learning problems, except for few FMQ heuristic based algorithms. The idea of FMQ heuristic is to take into consideration how frequent the action receives its maximum reward along with its individual Q-Value. The proposed algorithms still had coordination problems as per the results from their simulations. The authors of the paper try to address the coordination issues in social learning with their algorithm.

When agents perform social learning, they are assigned different roles namely row player & column player. This can be thought as something like when playing soccer two agents in the same team, one having the ball tries to move towards goal while the other tries to move towards a supporting position for the ball to be passed to the latter. So agents interact in pairs in each round and eventually try to come up with a optimal policy incrementally based on their experience after every round. The two effective factors in such kind of learning would be local interaction (where learning experience comes from its own group) and global observation (where each agent have access the experience of all other agents). The first one would have faster computation the latter would have better results but computationally it would be infeasible. 

To trade off from the above factors they divide the entire population in to M groups and instead of keeping track of all other agents experience the agents can learn from experiences of agents in other groups having the same role. This type of learning is called Individual Action Learning (IAL). Here each row player can have access to experience of all other row players across M groups and their learning is based on their experience. In individual action learning each agent pick the action with maximum payoff of all elements in group and updates their Q-Values accordingly. Intuitively the Q-learning update incorporates the optimistic assumption from Q-Learning and the so far successful FMQ heuristic (since frequency of the action being chosen also plays a key role in the update).

The other variation to the above learning is the Joint Action Learning (JAL) in which, each agent can have access to the joint actions of its own group and other M groups. For an agent to achieve maximum payoff by choosing an action as it generally depends upon the interacting agents’ action as well. Imagine this to be if the agent in soccer is in a good position to pass the ball to the striker, but instead striker doesn't put in the goal due to improper choosing of its action, eventually the former agent’s payoff will be affected. So to keep track of such cases for an agent i the frequency of action a and b (interacting agents' action) would be recorded to see which combination has maximum payoff.

The authors conducted the experiments for both the above learning for the Climbing game and Penalty game under various circumstances like deterministic, partially stochastic and fully stochastic. Under deterministic case both IAL and JAL perform well and reach the optimal policy. Where as in Partial stochastic environment, JALs perform slightly better than IALs in convergence and in fully stochastic case clearly JALs outperform IALs. This is intuitively expected since JALs can identify which action pair is optimum where as in IALs the agent ignores the action performed by their partners and thus they cannot distinguish between noise from stochasticity of the game and the explorations of their interacting partners. Thus it takes more time for IALs to converge.