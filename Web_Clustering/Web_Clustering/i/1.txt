I would advocate for acceptance of this paper. I believe that it presents a novel approach to a real-time decision making process under partial observability and uncertainty in that it combines to approaches in a new way; POMDP planning and “optimize-while-execute”. This approach allows actions to be selected by an online POMDP planner while the agent is executing an action so that by the time the agent completes the current action, the next action is already available. The novelty of this approach is that the planning is done with is a time-constraint so that the next action is available when the execution engine is ready for the next action. Other approaches require either offline planning or do not provide the time constraint needed for the real-time system.

There are sections of the paper that could be improved to provide better clarity. While describing the approach, the general case is often intermingled with the specific case presented in the paper. Specifically, the general case would be able to handle a random number of target models, but the sample case of three models was often used when describing the general use of the approach. It may have been better to continue describing the approach generally until the specific case was described. Also, the phrase “only known at the beginning of the mission during flight” was used multiple times and it was not clear to me what this meant. It could mean that the information was discovered during the first phase of the flight or that it was provided and the beginning of the mission. It leads one to believe that the information was not needed or known later in the mission, which could not be true. For example, the searched target could have been provided at the beginning of the mission while discovery of the zones would have happened during the first phase of the mission. Both of these are needed throughout the rest of the mission as well. More clarity can also be provided as to how the POMDP model was generated at the beginning of the mission, including how rewards were assigned.

Technically, the paper seems sound. However, I do have concerns about two items in the paper. First, the paper states that experience from previous missions shows that the “actions are sufficiently precise to be considered as deterministic.” While this greatly simplifies the complexity of the POMDP model, in the real-world, I fear this could lead to errors and, depending on the nature of the mission, disastrous results. It may be necessary to build uncertainty of the actions into the model. Although, the author does reference other papers to support this conclusion when he states that “the complexity of solving POMDPs essentially comes from probabilistic observations rather than from probabilistic action effects (Sabbadin, Lang and Ravoanjanahary 2007).”

Second, the trials made were on very small models with few zones, models and heights. A real-world scenario would have a much larger state spaces and I am not confident the same performance results could be achieved in such a case. The experiments done to validate the approach only use similar techniques on the same small scenario. I believe an experiment on a larger world would have been appropriate.

I do believe the paper makes a good contribution to the community. It proposes a new approach to online, real-time planning using POMDPs and identifies areas where additional research can be performed to further improve the technique. I would recommend that the paper be accepted.