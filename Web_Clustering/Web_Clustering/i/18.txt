In “The Dynamics of Reinforcement Social Learning in Cooperative MultiAgent Systems”, authors Jianye Hao and Ho-fung Lueng investigate the multi-agent coordination problem with many agents under a social learning framework.  The authors then introduce two types of agents, Individual Action Learners (IALs) and Joint Action Learners (JALs).  IALs learn by ignoring the other agent involved and simply computing the Qvalue of each of its actions, while the JAL will take into account the actions of the other agent and compute its own qvalues accordingly.  Using a few types of games, including deterministic, partially stochastic, and fully stochastic, this paper sheds light into the performance of different Multiagent Systems under different environments.
The author does a good job of explaining how the JAL and IAL are an extension of classical Q-Learning algorithm.  The implementation of these frameworks as a modified Q-Learning algorithms seem perfectly sound. 
In the setup of the experiment, the authors describe an algorithm which takes two random agents, teams them together, until all agents are paired up.  They then run the experiment to gain experience and repeat the entire process (including random selection) for a number of rounds.  While the procedure behind this seems perfectly reasonable and correct, some of the wording of the paper was a bit ambiguous. The paper refers to selecting an agent as a “column player” or a “row player”.  This distinction at first shows no real purpose other than saying that the two agents were teamed up.  However, much later it shows that there is a use in this language in the presentation of the payoffs of the different actions of the games.  While it makes sense in retrospect, using that language without context in the beginning of the paper is somewhat confusing.  
The main draw of this paper is the experimental results given.  Some good results were found that could influence more research in this topic.  Some of these results conclude that:
-	In deterministic games, both IAL and JAL agents converge on the correct answer in the same number of iterations. 
-	JALs seem to excel in stochastic environments, whereas IALs failed to converge to the correct answer.
-	The value of M (the number of groups an agent can observe as well) can negatively influence performance if it is too low or too high.  If M is too low, the agent takes longer to come produce data for the q-learning algorithm. If M is too high, it can introduce noise to the problem which will adversely influence performance.
The authors also state that under a fully stochastic environment, the JAL agents were able to converge 100% of the time where the next best solution could only converge 90% of the time. This will no doubt prompt further review and research.  Other topics that would need to be researched would be the selecting an optimal value M for the problem.