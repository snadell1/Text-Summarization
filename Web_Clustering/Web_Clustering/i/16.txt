RL algorithms are often tweaked to fit specific problems, we tweak the state space representation, or other parameters of the algorithm to get optimal performance. But this approach is like over-fitting, less autonomous, and hence less intelligent. This paper proposes combining many such RL agents, whose parameters have been tweaked for performance on individual MDPs, and coming up with an ensemble Learner that will perform better than the individual learner in a generalized case.

The proposed algorithm is a sequential process. First there are many base learners, which learn the parameters for individual training MDPs. The Q-values of the meta learner are the weighted sum of the base learners. The task of the meta learner is to learn these individual weights to get an efficient combination of the base learners. This is accomplished by optimizing the weights assigned to each of the base learner while trying the minimize the error metric( in this case the error metric is the square of the difference in the Q-values of the successive state-action pairs).
 
The author of the paper substantiates this idea by  running this algorithm for the mountain car problem. Though the algorithm does have an interesting approach to reinforcement learning, it does leave some open questions - the MDPs drawn seem to be uniform, which goes against the very spirit of the paper which is trying to solve the problem of solutions that over-fit the problem. What if the state space of the different MDPs are different, how will the meta learner behave if it encounters a state which none of its base learners had encountered in their learning process? In the example considered where we had 10 training MDPs, the process involved filtering some of the parameters of the learned agents and coming down to 4 parameters for the base learners of the meta learner, what was the basis for this process, when do you say the parameters are equivalent. The algorithm is simple and easy to understand and TD-learning approach with the meta learner makes the algorithm less computationally expensive compared to other probable meta learning approaches.