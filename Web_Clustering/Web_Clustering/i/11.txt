The paper basically outlines a novel approach to reinforcement learning, where the agent learns not one but a set of optimal policies. In a dynamic environment a single policy might not always be good, the agent should be able to adapt to external events and the agent should be able to perform a task in different ways so that depending on the changing external environment, the agent can choose the policy that is most optimal in the given scenario. It is in this spirit that the paper presents a method to evaluate multiple optimum policies using policy search through non parametric Bayesian Learning.

The DIPOLE algorithm uses the Infinite Guassian Mixture Model(IGMM, an extension of a Dirichlet Process) to encode the multiple policies. At every trial a set of policies are drawn from this IGMM, the policy is evaluated, using any of the Reinforcement Learning Policy Search algorithms, and its parameters updated based on the the rewards obtained. After a certain number of trials the DP clustering algorithm is run to reorganize the updated policy, and build a statistical model of the best policies.

The approach of coming up with a set of policies rather than a single policy is quite novel and will possibly be useful in a dynamic changing environment. But given a problem instance where we are expected to take an action, we are back to the problem of how to choose a policy from a set of optimum policies. We probably have to again do a policy search over this reduced space of optimum policies, which is an additional cost for the agent.
