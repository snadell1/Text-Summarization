In this paper, the authors introduce AstonTAC, an energy broker developed based on MDPs. The novelty of this work lies in mapping the problem into a MDP formulation. The paper is very well written with emphasis on explaining the competition setup. The basic logic behind their intelligent broker agent is as follows:

The competition simulator PowerTAC provides a range of information pertaining to the supply and demand of energy including public information such as clearing price, current weather, forecasted weather etc., and private information such as published tariffs, information about bids and asks in the wholesale market etc. The intelligent agent is expected to make the maximum profit while trying to maintain low energy imbalance between supply and demand for 1440 simulated hours.Initially, information for 360 hours were provided.

Using the past information, values such as the energy consumed by the consumer, energy produced by the consumer and clearing price in the wholesale market were predicted using Non-Homogeneous Hidden Markov Models (NHHMM). As each of these values are continuous, they were partitioned into discrete states for prediction by NHHMM using k-means clustering. However, no justification was given on why k was set to 20. Once partitioned, the transition probabilities between these (hidden) states were calculated. These probabilities was time dependent. Therefore, a 20*20 transition matrix was created for each of the 24 hours in the day. However, it might be interesting to see a day-wise transition matrix as well to take into account, variations in demand and supply during weekdays and weekends. Using the transition matrix, the values for the future hours (upto 24 hours) were predicted. Once the actual values for the hours are received, the hidden states and transition matrix are updated with the new information.

After obtaining the predicted energy demand (using the predicted energy consumed by the consumer and energy produced by the consumer) and the predicted clearing price, an MDP model is built to decide th optimum amount to bid at the wholesale market. This is a finite horizon MDP as it is a 1-day ahead predictor. The states are represented by number of hours ahead, energy quantity needed and predicted clearing price. Actions are a combination of bidding price and bidding volume. What I found most interesting was how the reward function was formulated. The agent gets an immediate reward for bidding at a low price and a delayed reward for energy balancing after 24 hours, which is quite intuitive. The reward was formulated such that it encourages buying at low price and maintaining low energy imbalance.

Once the MDP is formulated, the optimum policy is obtained using a combination of the usual finite horizon MDP and state-action rewards MDP. A thorough evaluation was performed. Reasonable explanation as to why CrocodileAgent and SotonPower obtained a better normalized score was given. The authors clearly showed the performance of their agent with respect to bidding volume and market clearing price for a particular game. The AstonTAC appears to nicely adapt itself to the clearing price, that is, when the clearing price is low, the agent bids in large volumes and vice-versa. Further, the authors also show how the predicted energy consumption and the actual energy consumption are very similar.

Overall, I find this work to be excellent in terms of their approach and evaluation with some scope for further improvement.