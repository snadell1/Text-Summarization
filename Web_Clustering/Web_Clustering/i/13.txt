Paper deals with solving the Reinforcement Learning problem in a Decision-theoretic approach. With unknown environment, a prior distribution is considered with underlying MDP environment such that expected utility is the integration over the MDP distribution. Using correlated state features which enables the less usage of data makes the work of authors differ from conventional independent state distribution.Environment dynamics dealt in this paper is made linear. The usage of Thompson Sampling instead of Most likely or expected model(i.e MDP heuristics or Monte Carlo Sampling) along with correlated state features ( unlike conventional state distribution independent gaussian process) is the primary enhancement made by the authors to deal with the Decision Theoretic approach. The beauty of prior distribution is that expected utility is calculated over the total interval of MDP distribution which makes expected value to converge.

Dynamics are made linear by assuming Gaussian noise. Conjugate pairs are used for parameters as we donot assume independent outcomes. The model is parameterized by design matrix Ai which is matrix - normal distribution and covariance matrix Vi which is inverse-wishart which together constitutes the prior distribution of environment.Expected MDP or Monte-Carlo Sampling are not used as they ignore shape of posterior distribution or their optimization is hard respectively. instead Thompson Sampling is used for policy optimization which is a special case of single expected MDP which can alternatively stated as monte-carlo sampling with k=1.

Fitted Value Iteration(FVI) and Least Square temporal difference(LSTD) is used as ADP methods.Choosing a random policy, model is sampled from the posterior distribution with above mentioned parameters. Stated ADP methods are used for calculating new policy. This new policy is tested in real Environment. The method proposed(LBRL) is tested against LSPI. LBRL with LSTD yielded a better result when compared to LSPI in both experiments of Inverted pendulum and Mountain Car in terms of stability with performance being similar and LBRL-FVI is more volatile.

Authors have taken care that each and every step is filled with best alternative. Mathematical enhancements are made to yield good result. so Overall evaluation is strong accept. But results shows that algorithm stabilised over LSPI but the performance is similar,because of which I didnt suggest it for Best Paper. Paper sounds good in technical aspects depicting every mathematical and technical attributes, but lacks in full novelty. The level is presentation is good.