Decentralized POMDPs are a powerful model for stochastic multi-agent coordination problems. Most work/ solutions in this area have involved assumptions exogenous to this domain including centralized solving and assumptions of prior knowledge. In order to make solution methods more accurately model the problem domain the solution methods must be model free and decentralized themselves. In Bikramjit Banerjee's "Pruning for Monte Carlo Distributed Reinforcement Learning in Decentralized POMDPs "Dr. Banerjee proposes a novel method by which existing alternating decentralized q-learning agents can vastly improve their efficiency and sample complexity through the use of exploration space pruning.

Generating optimal solutions even for time horizon based Dec-POMDPs is incredibly complex (in both space and time). This owes in part to the issue of multiple agent coordination building on top of the issues presented by traditional POMDPs. The definition of POMDPs is changed/ expanded to include n, the number of agents, A is redefined to be th product of each agents possible actions, the transition system P(s'|s,a) will operate with a as an element of the new (product) A, and omega is changed to the product of the two agents observations (with the observational model O changing to reflect this.) Thus the reward, transition and observation models are defined over joint actions/ observations and enforce agent coordination. The goal then of a dec-POMDP problem is to find a policy for each agent (joint policy) that maximizes total expected reward over |T| steps given the agents cannot communicate their observations and actions to each other.

This paper proposes a refinement on the existing MCQ-Alt algorithm. It seeks to lower sample complexity by pruning exploration space to highlight more interesting/ important areas for investigation. This method is inspired by the common observation that policy often converges before valuation.

The original implementation of MCQ- Alt records immediate rewards and history transitions at every history encountered providing samples for R* and H*. The samples are incorporated into running averages to create approximation functions R hat and H hat. Because it models intermediate functions the method is often referred to as semi-model based. The Q-learning agent itself uses an exploration policy that divides histories into "known" and "not known". The agent merely follows a greedy policy in known histories but takes the least frequently used action in not known histories. The agents take turns exploring (while the other agent operates on its current approximate policy).

Iterative MCQ-Alt is the refinement proposed that creates a pruning criterion for the exploration tree. Ideally this will adaptively discard actions (at different levels of the experience tree) from further consideration without affecting the confidence in the Q-values that yield the best response policy. This is achieved by allowing actions lower in the tree to be removed with relatively fewer samples than higher ones.

Banerjee put his method to the test against standard MCQ-Alt through a battery of standard Dec-POMDP benchmark environments (DEC-TIGER (Nair et al. 2003), RECYCLING-ROBOTS (Amato, Bernstein, and Zilberstein 2007), BOX-PUSHING (Seuken and Zilberstein 2007) and MARS-ROVERS (Amato and Zilberstein 2009). First agents were allowed to learn initial policy by concurrent reinforcement learning (while ignoring observations) to set up an exploratory baseline for the two algorithms to build on. The agents took turns to learn their best responses to the other policies using MCQ-ALT and IMCQ-ALT. The results of the empirical testing showed high degrees of pruning in each environment with Recycling robots showing pruning of up to 70-91% with 0 policy error.

While this algorithm will no doubt have implications far outside of the original benchmarks it can be seen more as a refinement of previous work rather than blazing a new path. Future research possibilities include a more judicious use of samples and possibly related exploration of rare histories. That said, the algorithm will probably find direct application in cooperative/ coordinated robotics (manufacturing) as well as coordinated team simulation in virtual environments and/or video games. Further research should be done on its online performance.
