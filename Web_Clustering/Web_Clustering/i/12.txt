The paper focuses on the Bayesian approach to solving Reinforcement Learning (RL) problems. In RL, the agent tries to find the policy (π) for the unknown environment (µ) which maximizes the expected utility (Eµ,π U). Here, optimizing with respect to the policy (π) is not possible since the environment is unknown. Hence, in the Bayesian approach to RL, a prior distribution (ξ) over the environment (µ) is chosen such than ξ (µ) is the belief that µ is the true environment. The expected utility over the real environment (which is unknown) is then replaced with the expected utility under the priors (Eξ,π U).

Since the exact solution for the Bayesian RL problem is intractable, the authors use Thompson Sampling as an approximate method for solving this problem where they sample a model from the posterior distribution and use the policy obtained from the model for some period. For each sampled model, the optimal policy is calculated using Approximate Dynamic Programming (ADP) and the policy is executed on the real environment. This ensures efficient exploration as the model does not choose the most likely (expected) transition dynamics to calculate policies and value functions.     

The clarity of presentation and technical soundness of this paper is excellent. The authors have provided detailed explanations for their algorithm design choices and have clearly stated the pitfalls in using methods such as Expected MDP and full Monte Carlo Sampling and the advantages of Thompson Sampling over these methods. 

The novelty of contribution is good and the authors have explained how their approach is better than using Gaussian Process Models in terms of scalability to real world problems and accounting for correlations in state features. The paper is suitable for a Full Presentation and I strongly believe it should be accepted. However, the evaluation results presented are not convincing enough to show that LRBL performs significantly better than LSPI and hence there needs to be further evaluation on more domains before a conclusion on this issue can be drawn. For this reason, I am not nominating this paper for the Best Paper Award.